# -*- coding: utf-8 -*-
"""Dueling DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VcdNt6INr0NoKczsD3vE-hJ1gDP637B3
"""

import torch
from torch import nn
import numpy as np
import torch.functional as F
from collections import namedtuple
import random
from torchvision import models
import math
from PIL import Image

import torch.nn.functional as F
from scipy.special import softmax

from torch.nn.modules.activation import ReLU
class Network(nn.Module):
  def __init__(self):
    super(Network, self).__init__()
    self.layers=nn.ModuleList([
    nn.Linear(21,1)

    ])
    self.advantage = nn.Linear(1,18)
    self.value=nn.Linear(1,1)
  def forward(self,state):
    x=state
    for i in range(len(self.layers)):
      x=self.layers[i](x)
      x=nn.ReLU()(x)
    adv=self.advantage(x)
    v=self.value(x)
    return v + (adv - torch.mean(adv, dim=1, keepdim=True))
  def get_advantage(self,state):
    x=state
    for i in range(len(self.layers)):
      x=self.layers[i](x)
      x=nn.ReLU()(x)
    adv=self.advantage(x)
    return adv

Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward','done'))
class ReplayMemory:

    def __init__(self, capacity,batch_size):
        self.capacity = capacity
        self.memory = []
        self.position = 0
        self.batch_size=batch_size

    def push(self, *args):
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = Transition(*args)
        self.position = (self.position + 1) % self.capacity

    def sample(self):
      return random.sample(self.memory, self.batch_size)


    def __len__(self):
        return len(self.memory)

class Agent:
  def __init__(self):
    self.losses=[]
    self.q=Network()
    self.target=Network()
    self.target.eval()
    self.update_target()
    self.gamma=0.99
    self.optimizer =torch.optim.Adam(self.q.parameters(),lr=0.005)
    self.eps=1
    self.f=0

  def action(self,state):
    if np.random.random()>self.eps:
       q=self.q(state).argmax()
       return int(q.detach().cpu().numpy())
    else:
      actions = self.q.net.get_advantage(np.array([state]))
      action = np.argmax(actions)
      return action

    # return np.random.choice(len(policy), p=policy)
  def greedy_action(self,state):
      q=self.q(state).argmax()
      return int(q.detach().cpu().numpy())
  def update_target(self):
    self.target.load_state_dict(self.q.state_dict())

  #for saving the best weight
  def save(self,address):
    torch.save(self.q,address+"/model.pth")
  def save_log(self):
    np.save("loss.npy",self.losses)
  #loading the best weight
  def load(self,address):
    self.q=torch.load(address+"/model.pth")

  #update Q network
  def update(self,buffer):
    if len(buffer)<buffer.batch_size:
      return
    batch=buffer.sample()#taking some sanples
    batch = Transition(*zip(*batch))
    batch_size=buffer.batch_size
    states=batch.state
    next_states=batch.next_state
    rewards=torch.FloatTensor(batch.reward)
    dones=torch.FloatTensor(batch.done)
    actions=batch.action
    actions = torch.tensor(actions)
    q=self.q(torch.FloatTensor(states))
    # q = q[torch.arange(batch_size),actions]
    q=q.gather(1, actions.unsqueeze(1))
    q_next= self.q(torch.FloatTensor(next_states))
    next_actions=torch.argmax(q_next, dim=1)
    q_next= self.target(torch.FloatTensor(next_states))
    # q_next= q_next[torch.arange(batch_size), next_actions]
    q_next=q_next.gather(1, next_actions.unsqueeze(1))
    q_next=q_next.detach()
    target=rewards.unsqueeze(1)+self.gamma*q_next*(1-dones.unsqueeze(1))
    # target=rewards.unsqueeze(1)+self.gamma*q_next
    l=torch.nn.MSELoss()
    loss=l(q,target)
    self.optimizer.zero_grad()
    loss.backward()
    # for param in self.q.parameters():
    #   param.grad.data.clamp_(-1, 1)
    self.optimizer.step()
    self.f+=1
    self.losses.append(loss)
    #updating target network
    if self.f==200:

      self.update_target()
      self.f=0

agent=Agent()
BATCH_SIZE=32
buffer=ReplayMemory(100000,BATCH_SIZE)

for episode in range(21316):
  for step in range(len(all_possessions[episode])-1):
    state=all_possessions[episode][step][2:23]
    action=all_possessions[episode][step][1].astype(np.int64)
    next_state=all_possessions[episode][step+1][2:23]
    reward=all_possessions[episode][step][42]
    done=(step==(len(all_possessions[episode])))
    buffer.push(state, action, next_state, reward,done)
    agent.update(buffer)


  agent.eps-=1/21316

states = []
rewards=[]
actions=[]
for episode in range(21316):
  for step in range(len(all_possessions[episode])):
    state=all_possessions[episode][step][2:23]
    states.append(state)
    action=all_possessions[episode][step][1].astype(np.int64)
    actions.append(action)
    reward=all_possessions[episode][step][42]
    rewards.append(reward)

states = np.array(states)

state = torch.from_numpy(states).float()

adv=agent.q.get_advantage(state)

actions = np.array(actions)

Advantage=[]
for i in range(82370):
  a=actions[i]
  advantage=adv[i][a]
  advantage=advantage.detach().numpy()
  Advantage.append(advantage)

Advantage=np.array(Advantage)

import matplotlib.pyplot as plt
plt.subplot(2,1,1)
plt.scatter(np.arange(200),Advantage[0:200])
plt.subplot(2,1,2)
plt.scatter(np.arange(200),rewards[0:200])

indexx =[]
for i in range(21316):
  for j in range(len(all_possessions[i])):
    Index = all_possessions[i][j][7]
    indexx.append(Index)

indexx = np.array(indexx)